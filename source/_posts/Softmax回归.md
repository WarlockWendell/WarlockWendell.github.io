---
title: Softmax回归
date: 2021-11-29 22:50:31
tags: [深度学习,PyTorch]
---

其实Softmax更多是用在分类问题上的,但是其本质上是一个线性模型.

<!--more-->

# 1. 原理

回归问题输出的是一个连续值, 分类问题需要输出的是一个类别,但是在一定的模型下,分类问题可以理解为输出各个类别的概率,这样其标签值就是一个one-hot编码,输出概率也是一个连续值,所以也像一个回归问题的解决思路? 不同的是, 回归问题关心预测出来的值的绝对大小,  而分类问题只关心相对大小, 并不关心绝对大小,或者说绝对大小没有那么重要.

其过程大概是

- 对类别进行编码
- 训练
- 用输出的最大值作为预测: $\hat y = \mathop{\arg\min}\limits_{i} {o_i}$, 这里的输出还是线性模型的输出

但是对于一个线性模型来说, 其输出的大小不是固定的, 可能是负的, 还可能大于1,各个类别加起来也不是1, 所以需要一个Softmax层

但其实 加了softmax,输出的下标还是那个下标, 到底有啥意义?

我的理解是

- 方便打label了: 因为如果是线性模型一样,那样不加限制的输出, 那么一开始我怎么打label呢,还是用one-hot编码? (其实好像也不是不可以哦, 用MSE Loss, 让每一个输出去拟合0和1也不是不可以,YOLO 前两代应该就是这么干的)

  > 其实数学证明里看来, 加softmax,用交叉熵损失函数 和 不加softmax 用MSE Loss, 本质上都是在做最大似然估计呀, 这个真的有用嘛?

- 使之成为一个合理的概率分布了, 其实这是一个很鸡肋的说法, 因为softmax让它到0-1之间了,但是并没有改变相对大小关系. 合理的解释是 是输出概率化, 更好解释? 或者说更好设定参数?

- softmax函数的性质很好,方便求导

- > 并没有真正理解这个



公式

$O=XW + b$

$\hat Y = softmax(O)$

损失函数用交叉熵损失函数

$l(\hat y, y) = - \sum_{i=0}^{n}\hat y_i log(y_i)$

带入标签之后就是

$l(\hat y , y) = -log(\hat y_y)$



# 2. 实现过程中的一些坑

- 使用`nn.CrossRntropyLoss()`时,输入不是归一化之后的变量, 因为为了避免数值运算的问题, 交叉熵函数在实现时已经自己做了softmax了

  
